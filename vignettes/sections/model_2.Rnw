\subsection{model\_2 to analyse $G \times E$ interaction in the network of farms }
\label{model_2}

At the \textbf{network level}, there is a large germplasm $\times$ environment combinaisons that are missing, leading to a poor estimation of germplasm, environment and interaction effects.
Hence, model\_2 was implemented (section~\ref{section_model2}).

For model\_2, it gave nice results with 75 environments and 120 germplasms present in at least two environments (95\% of missing $G \times E$ combinaisons) \citep{riviere_hierarchical_2016}.


\subsubsection{Theory of the model}
The model is describe in \citet{riviere_hierarchical_2016}.

The phenotypic value $Y_{ij}$ for a given variable $Y$, germplasm $i$ and environment $j$, was modeled as :

\begin{displaymath}
Y_{ij} = \alpha_{i} + \theta_{j} + \eta_{i}\theta_{j} + \varepsilon_{ij} ; \quad \varepsilon_{ij} \sim \mathcal{N} (0,\sigma^2_{e}),
\label{modele_gxe}
\end{displaymath}

for $i = 1,\ldots, I$ and $j = 1,\ldots, J$, where 
$I$ was the number of germplasms, 
$J$ was the number of environments,
$\alpha_{i}$ was the main effect of germplasm $i$,
$\theta_{j}$ was the main effect of environnment $j$,
$\varepsilon_{ij}$ was the residual and 
$\mathcal{N} (0,\sigma^2_{e})$ was the normal distribution with mean 0 and variance $\sigma^2_{e}$.
The interaction between germplasm $i$ and environment $j$ was divided into a multiplicative term $\eta_{i}\theta_{j}$ and a remaining term that contributed to the residual $\varepsilon_{ij}$.

This model was written as :

\begin{equation}
Y_{ij}  = \alpha_{i} + \beta_{i} \theta_{j} + \varepsilon_{ij}; \quad \varepsilon_{ij} \sim \mathcal{N} (0,\sigma_{\varepsilon}),
	\label{model2}
\end{equation}

Where $\beta_{i} = (1 + \eta_{i})$ was the sensitivity of germplasm $i$ to environments.
This model is known as the Finlay Wilkinson model or as joint regression \citep{finlay_analysis_1963}.
Germplasm sensitivities quantified the stability of germplasm performances over environments.
The average sensitivity was equal to 1 so that a gemplasm with $\beta_{i} > 1$ ($\beta_{i} < 1$) was more (less) sensitive to environments than a germplasm with the average sensitivity \citep{nabugoomu_analysis_1999}.

Given the high disequilibrium of the data and the large amount of data, we decided to implement this model with a hierarchical Bayesian approach.
In the following, this Hierarchical Finlay Wilkinson model was denoted by HFW.

We used hierarchical priors for $\alpha_i$, $\beta_i$ and $\theta_j$ and a vague prior for $\sigma_{\varepsilon}$.

\begin{displaymath}
\alpha_{i} \sim \mathcal{N} (\mu,\sigma^2_{\alpha}), \quad 
\beta_{i} \sim \mathcal{N} (1,\sigma^2_{\beta}), \quad 
\theta_{j} \sim \mathcal{N} (0,\sigma^2_{\theta}), \quad 
\sigma^{-2}_{\varepsilon} \sim \mathcal{G}amma (10^{-6},10^{-6}),
\end{displaymath}

where $\mu$, $\sigma^2_{\alpha}$, $\sigma^2_{\beta}$ and $\sigma^2_{\theta}$ were unknown parameters.
The mean of $\beta_i$ was set to 1 \citep{nabugoomu_analysis_1999}.


Then, we placed weakly-informative priors on the hyperparmeters  $\mu$, $\sigma^2_{\alpha}$, $\sigma^2_{\beta}$ and $\sigma^2_{\theta}$:

\begin{displaymath}
\mu \sim \mathcal{N} (\nu,\nu^2), \quad 
\sigma_{\alpha} \sim \mathcal{U}niforme (0,\nu), \quad 
\sigma_{\beta} \sim \mathcal{U}niforme (0,1), \quad 
\sigma_{\theta} \sim \mathcal{U}niforme (0,\nu),
\end{displaymath}

where $\nu$ was the arithmetic mean of the data : $\nu = \sum_{ij} {Y_{ij}/n}$ where $n$ was the number of observations.
Uniform priors were used for $\sigma^2_{\alpha}$, $\sigma^2_{\beta}$ and $\sigma^2_{\theta}$ to reduce the influence of these priors on posterior results \citep{gelman__2006}.
The support of these priors took account of the prior knowledge that $\sigma^2_{\alpha}$, $\sigma^2_{\beta}$ and $\sigma^2_{\theta}$ were expected to be respectively smaller than $\nu$, 1, $\nu$. \\

Initial values for each chain were taken randomly except for $\mu$, $\sigma_{\alpha}$ and $\sigma_{\theta}$ whose initial values were equal to their posterior median from additive model (i.e. model \ref{model2} with $\forall i, \beta_{i}=1$). \\


The main parameter of interest were 
germplasm main effects ($\alpha_{i}, i = 1,\ldots, I$), 
environment main effects ($\theta_{j}, j = 1,\ldots, J$) and 
germplasm sensitivities ($\beta_{i}, i = 1,\ldots, I$).
For $\alpha_i$, the average posterior response of each germplasm over the environments of the network was considered:

\begin{displaymath}
\gamma_i = \alpha_i + \beta_{i} \bar{\theta},
\end{displaymath}
where
$\bar{\theta} = \sum_{}^{J} \theta_j/J$.

To simplify, the $\alpha_i$ notation is kept instead of $\gamma_i$ (i.e. $\alpha_i = \gamma_i$).
But keep in mind it has been corrected.

\subsubsection{Steps with \pack}

For model\_2, you can follow these steps (Figure \ref{function_relations}):

\begin{enumerate}
\item Run the model with \texttt{model\_2}
\item Check model outputs with graphs to know if you can continue the analysis with \texttt{check\_model}
\item Perform cross validation studies with \texttt{cross\_validation\_model\_2} in order to assess the quality of the model
\item Get mean comparisons for each factor with \texttt{mean\_comparisons} and vizualise it with \texttt{get\_ggplot}
\item Get groups of parameters for $\alpha$, $\beta$ and $\theta$ with \texttt{parameters\_groups} and visualise it with \texttt{get\_ggplot}
\item Predict the past with \texttt{predict\_the\_past\_model\_2} and vizualise it with \texttt{get\_ggplot}
\end{enumerate}

Let's get the data.
The values for $\alpha_i$, $\beta_i$, $\theta_j$ are the real value taken to create the dataset for y1.
This dataset is representative of data you can get in a PPB programme.

<<message=TRUE,cache=FALSE>>=
data(data_model_2)
head(data_model_2)
@

\subsubsection{Run the model}

To run model\_2} on the dataset, used the function \texttt{model\_2}.
You can run it on one variable.
Here it is on thousand kernel weight (tkw)

By default, \texttt{model\_2} returns posteriors for 
$\alpha_i$ (\texttt{return.alpha = TRUE}),
$\sigma_{\alpha}$ (\texttt{return.sigma\_alpha = TRUE}),
$\beta_i$ (\texttt{return.beta = TRUE}),
$\sigma_{\beta}$ (\texttt{return.sigma\_beta = TRUE}),
$\theta_j$ (\texttt{return.theta = TRUE}),
$\sigma_{\theta}$ (\texttt{return.sigma\_theta = TRUE}) and
$\sigma_{\epsilon}$ (\texttt{return.sigma\_epsilon = TRUE}).
You can also get $\epsilon_{ij}$ with \texttt{return.epsilon = TRUE}.

By default, DIC is not display, you may want this value to compare to other model (\texttt{DIC = TRUE}).
DIC criterion is a generalization of the AIC criterion that can be used for hierarchical models \citep{spiegelhalter_bayesian_2002}.
The smaller the DIC value, the better the model \citep{plummer_penalized_2008}.

<<message=TRUE,cache=FALSE>>=
# out_model_2 = model_2(data = data_model_2, variable = "y1", return.epsilon = TRUE)

# Run additive model ...
# Compiling model graph
# Resolving undeclared variables
# Allocating nodes
# Graph information:
#   Observed stochastic nodes: 2379
# Unobserved stochastic nodes: 228
# Total graph size: 9834
# 
# Initializing model
# 
# |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
# |**************************************************| 100%
# |**************************************************| 100%
# Run FWH model ...
# Compiling model graph
# Resolving undeclared variables
# Allocating nodes
# Graph information:
#   Observed stochastic nodes: 2379
# Unobserved stochastic nodes: 386
# Total graph size: 14913
# 
# Initializing model
# 
# |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
# |**************************************************| 100%
# |**************************************************| 100%
# |**************************************************| 100%
# 

load("./data_PPBstats/out_model_2.RData")
@

It may be useful to see which germplasm were not use in the analysis because they were in only one environment.

<<message=TRUE,cache=FALSE>>=
out_model_2$germplasm.not.used
@

\subsubsection{Check and visualize model outputs}

\paragraph{Check the model}

Once the model is run, it is necessary to check if the outputs can be taken with confidence. 
This step is needed before going ahead in the analysis (in fact, the MCMC object used in the next functions must come from \texttt{check\_model}!).


<<message=TRUE,cache=FALSE>>=
# out_check_model_2 = check_model(out_model_2)

# The Gelman-Rubin test is running for each parameter ...
# The two MCMC for each parameter converge thanks to the Gelman-Rubin test.

load("./data_PPBstats/out_check_model_2.RData")
@

\texttt{out\_check\_model\_2} is a list containing three elements:

\begin{itemize}
\item \texttt{MCMC} : a data fame resulting from the concatenation of the two MCMC for each parameter. This object can be used for further analysis. There are as many columns than parameters and as many rows than iterations/thin (the thin value is 10 by default in the models).
<<message=TRUE,cache=FALSE>>=
dim(out2$MCMC)
@

\item \texttt{MCMC\_conv\_not\_ok} : a data fame resulting from the concatenation of the two MCMC for each parameter for environment where  some parameters did not converge for mu and beta

\item \texttt{model2.presence.abscence.matrix} : a matrix germplasm x environment with the number of occurence in the data used for the model (i.e. with at least two germplasm by environments.)

\item \texttt{data\_ggplot} a list containing information for ggplot:
  \begin{itemize}
  \item alpha
  \item beta
  \item theta
  \item epsilon
  \end{itemize}

\end{itemize}

\paragraph{Visualize outputs}

Once the computation is done, you can visualize the results with \texttt{get\_ggplot}
<<message=TRUE,cache=FALSE>>=
p_out_check_model_2 = get_ggplot(out_check_model_2)
@

\texttt{p\_out\_check\_model\_2} is a list with 4 elements:

\begin{itemize}
\item \texttt{alpha\_i} : distribution of each alpha_i. There are as many graph as needed with \texttt{nb\_parameters\_per\_plot} \texttt{alpha\_i} per graph.
<<message=TRUE,cache=FALSE>>=
p_a = p_out_check_model_2$alpha_i
names(p_a)
p_a$`1`
@

\item \texttt{beta\_i} : distribution of each beta_i. There are as many graph as needed with \texttt{nb\_parameters\_per\_plot} \texttt{beta\_i} per graph.
<<message=TRUE,cache=FALSE>>=
p_a = p_out_check_model_2$beta_i
names(p_a)
p_a$`1`
@

\item \texttt{theta\_j} : distribution of each theta_j. There are as many graph as needed with \texttt{nb\_parameters\_per\_plot} \texttt{theta\_j} per graph.
<<message=TRUE,cache=FALSE>>=
p_a = p_out_check_model_2$theta_j
names(p_a)
p_a$`1`
@

\item \texttt{epsilon\_ij} : standardised residuals distribution.
If the model went well it should be between -2 and 2.
<<message=TRUE,cache=FALSE>>=
p_out_check_model_2$epsilon_ij
@


\item \texttt{mcmc\_not\_converge\_traceplot\_density} : a list with the plots of trace and density to check the convergence of the two MCMC only for chains that are not converging thanks to the Gelman-Rubin test. 
If all the chains converge, it is NULL
<<message=TRUE,cache=FALSE>>=
p_out_check_model_2$mcmc_not_converge_traceplot_density
@

\end{itemize}


Just for fun, you compare the posterior medians and the arithmetic means for the $\alpha_i$'s.

<<message=TRUE,cache=FALSE>>=
MCMC = out_check_model_2$MCMC
effects = apply(MCMC, 2, median)
alpha_i_estimated = effects[grep("alpha\\[",names(effects))]
names(alpha_i_estimated) = sapply(names(alpha_i_estimated), function(x){  
sub("\\]", "", sub("alpha\\[", "", x)) } )
 
alpha_i = tapply(PPBdata2$alpha_i, PPBdata2$germplasm, mean, na.rm = TRUE)
 
check = cbind.data.frame(alpha_i = alpha_i, alpha_i_estimated = alpha_i_estimated[names(alpha_i)])
@

Let’s have a look at the relation between both values.

\begin{figure}[H]
<<message=TRUE,cache=FALSE,out.width=".6\\textwidth">>=
p = ggplot(check, aes(x = alpha_i, y = alpha_i_estimated))
p + stat_smooth(method = "lm") + geom_point()
@
\end{figure}



\subsubsection{Perform cross validation studies}

This step is useful to assess the quality of the model.
This step is higly computing consuming as model\_2 is run as many time as there is value of $Y_{ij}$ (i.e. number of rows of the data set).

The complete cross validation is done with \texttt{cross\_validation\_model\_2}: 
each Value of $Y_{ij}$ is estimated by the entire data set without this value.

The convergence is not check for each validation. 
If the parameters in the FWH converge, then it is assumed that the FWH in the cross validation converge as well.

The model is run on dataset where germplasms are in three environments at least so the smallest data set where the cross valisation is run has germplasms present in two environments at least. 

You may parallelise to gain time with the \texttt{mc.cores} argument of the function.

The number of iterations is set to 100 000 but you can change it with the \texttt{nb\_iterations} argument.

Here it is bad as only 10 iterations for the 5 first $Y_{ij}$ have been done to save computing time ...

<<message=TRUE,cache=FALSE>>=
# out_cv = cross_validation_model_2(PPBdata2, "y1", nb_iterations = 10)

load("./data_PPBstats/out.cv.RData") # to save lots of time

head(out_cv)
@

The outputs are visualized with \texttt{get\_ggplot}

<<message=TRUE,cache=FALSE>>=
p_out_cv = get_ggplot(out_cv)
@

\texttt{p\_out\_cv} is a list of two elements:
\begin{itemize}
\item \texttt{plot} : plot estimated.value = f(observed.value). 
The probability mean = 0 is display.


The percentage of confidence is calculated with a t-test:

\begin{displaymath}
t = \frac{m - 0}{s/\sqrt{N}}
\end{displaymath}
with,

$N$ the number of observations in the data set,

$m = \frac{1}{N} \sum\limits_{n=1}^N Y_{n} - \hat{Y_{n}}$, the average bias

$s = \sqrt{\frac{1}{N-1} \sum\limits_{n=1}^N (Y_{n} - \hat{Y_{n}})^2}$, the standard deviation of the bias

$t$ follows a Student distribution with $N-1$ degree of freedom.

The percentage of confidence (i.e. the probability $H0$: the bias is equal to zero) comes from this distribution.

<<message=TRUE,cache=FALSE>>=
p_out_cv$plot
@

\item \texttt{regression} : output of the model observed.value = a x estimated.value + b
<<message=TRUE,cache=FALSE>>=
p_out_cv$regression
@

\end{itemize}


\subsubsection{Get and visualize mean comparisons}

\paragraph{Get mean comparisons}

For mean comparisons of parameters, it is the same method that presented in section \ref{mean_comp}.
It can be done for $\alpha_i$, $\beta_i$ and $\theta_j$.

<<message=TRUE,cache=FALSE>>=
# model_2_alpha = mean_comparisons(out_check_model_2, parameter = "alpha")
# model_2_beta = mean_comparisons(out_check_model_2, parameter = "beta", precision = 0.05)
# model_2_theta = mean_comparisons(out_check_model_2, parameter = "theta", precision = 0.05)

load("./data_PPBstats/model_2_alpha.RData")
load("./data_PPBstats/model_2_beta.RData")
load("./data_PPBstats/model_2_theta.RData")
@



\paragraph{Vizualise mean comparisons}

To see the output, use \texttt{get\_ggplot}.

There are as many graph as needed with \textttt{nb\_parameters\_per\_plot} parameters per graph.

For \texttt{ggplot.type = "barplot"},
Letters are displayed on each bar. Parameters that do not share the same letters are different regarding type I error (alpha) and alpha correction. 
The error I (alpha) and the alpha correction are displayed in the title. 
alpha = Imp means that no differences were possible to find.

For \texttt{ggplot.type = "biplot-alpha-beta"}, the biplot with $\alpha_i$ on the x axis and $\beta_i$ on the y axis.

<<message=TRUE,cache=FALSE>>=
p_a = get_ggplot(model_2_alpha, ggplot.type = "barplot")
names(p_a$alpha)
p_a$alpha$`1`
@

<<message=TRUE,cache=FALSE>>=
p_b = get_ggplot(model_2_beta, ggplot.type = "barplot")
names(p_b$beta)
p_b$beta$`1`
@

It is interessting to compare genetic effect versus sensibility to interaction.
A germplasm with an high genetic effect and a low sensitivity to interaction (i.e. close to 0) may be a good candidate to sown.

<<message=TRUE,cache=FALSE>>=
p_ab = get_ggplot(model_2_alpha, model_2_beta, ggplot.type = "biplot-alpha-beta")
p_ab$`1`
@


<<message=TRUE,cache=FALSE>>=
p_t = get_ggplot(model_2_theta, ggplot.type = "barplot")
names(p_t$theta)
p_t$theta$`9`
@



\subsubsection{Get and vizualise groups of parameters}

\paragraph{Get groups of parameters}

In order to cluster environments or germplasms, you may use mulivariate analysis on a matrix with several variables in columns and parameter in rows.

This is done with \texttt{parameter\_groups} which do a PCA on this matrix.

Clusters are done based on Average silhouette method for k-means clustering as explained here \url{http://www.sthda.com/english/wiki/determining-the-optimal-number-of-clusters-3-must-known-methods-unsupervised-machine-learning#r-codes-1}

The hcluster are computed with the function \texttt{hclust(dist())}

The kmeans are computed with the function \texttt{kmeans()}

Lets' have an example with three variables.

First run the models

<<message=TRUE,cache=FALSE>>=
# out_model_2_y1 = model_2(PPBdata2, variable = "y1")
# out_model_2_y2 = model_2(PPBdata2, variable = "y2")
# out_model_2_y3 = model_2(PPBdata2, variable = "y3")

load("./data_PPBstats/out_model_2_y1.RData")
load("./data_PPBstats/out_model_2_y2.RData")
load("./data_PPBstats/out_model_2_y3.RData")
@

Then check the models

<<message=TRUE,cache=FALSE>>=
# c_m2_y1 = check_model(out_model_2_y1)
# The Gelman-Rubin test is running for each parameter ...
# The two MCMC for each parameter converge thanks to the Gelman-Rubin test.

# c_m2_y2 = check_model(out_model_2_y2)
# The Gelman-Rubin test is running for each parameter ...
# The two MCMC for each parameter converge thanks to the Gelman-Rubin test.

# c_m2_y3 = check_model(out_model_2_y3)
# The Gelman-Rubin test is running for each parameter ...
# The two MCMC for each parameter converge thanks to the Gelman-Rubin test.

load("./data_PPBstats/c_m2_y1.RData")
load("./data_PPBstats/c_m2_y2.RData")
load("./data_PPBstats/c_m2_y3.RData")
@

Then run the function on alpha. 
It can also be done on beta or theta.

<<message=TRUE,cache=FALSE>>=
out_parameter_groups = parameter_groups(list("y1" = c_m2_y1, "y2" = c_m2_y2, "y3" = c_m2_y3), "alpha")
@

\texttt{out\_parameter\_groups} is list of two elements:

\begin{itemize}
\item\texttt{ obj.pca} : the PCA object from FactoMineR::PCA
\item \texttt{clust}, a list of four elements:
\begin{itemize}
  \item \texttt{data} : the matrix with variables in column and effect in row, from wich have been computed the PCA
  \item \texttt{nb.k} : number of cluster
  \item \texttt{hc} : hcluster
  \item \texttt{km} : kmeans
  \end{itemize}
\end{itemize}

\paragraph{Visualize groups of parameters}

Visualize outputs with \texttt{get\_ggplot}

<<message=TRUE,cache=FALSE>>=
ppg = get_ggplot(out_parameter_groups)
@

\texttt{ppg} is list of two elements:

\begin{itemize}

\item \texttt{pca} : a list with three elements on the PCA on the group of parameters :
  \begin{itemize}
  
  \item \texttt{composante_variance} : variance caught by each dimension of the PCA
  <<message=TRUE,cache=FALSE>>=
  ppg$pca$composante_variance
  @

  \item \texttt{ind} : graph of individuals
  <<message=TRUE,cache=FALSE>>=
  ppg$pca$ind
  @

  \item \texttt{var} : graph of variables
  <<message=TRUE,cache=FALSE>>=
  ppg$pca$var
  @

  \end{itemize}

\item \texttt{clust}
  \begin{itemize}
  
  \item \texttt{nb_k} : output from \texttt{factextra::fviz_nbclust(data, kmeans, method = "silhouette")}
  See \texttt{?factoextra::fviz_nbclust} for more details
  <<message=TRUE,cache=FALSE>>=
  ppg$clust$nb_k
  @

  \item pca : output from \texttt{factextra::fviz_cluster(km.res, data, frame.type = "norm")}
  See \texttt{?factoextra::fviz_cluster} for more details
  <<message=TRUE,cache=FALSE>>=
  ppg$clust$pca
  @

  \end{itemize}

\end{itemize}

A farmer may find a germplasm that behaves well according to informations from model\_1 (section \ref{section_model1}) in a farm that shares its cluster.



\subsubsection{Predict the past}

In order to choose a new germplasm to test on his farm, a farmer may choose a germplasm according to the value it would have obtained on his farm.

For example for \texttt{"loc-6:year-5"}
<<message=TRUE,cache=FALSE>>=
ptp = predict_the_past_model_2(out_check_model_2, env = "loc-6:year-5")
@

\texttt{ptp} can be handle exacly as \texttt{out\_model\_1}.

<<message=TRUE,cache=FALSE>>=
m_ptp = mean_comparisons(ptp)

p = get.ggplot(m_ptp, ggplot.type = "barplot")

data("data_version")
p = get.ggplot(m_ptp, data_version = data_version, ggplot.type = "barplot")

p = get.ggplot(m_ptp, ggplot.type = "score")

p = get.ggplot(m_ptp, ggplot.type = "interaction")
@

